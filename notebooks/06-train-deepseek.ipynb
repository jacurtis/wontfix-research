{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-09T04:49:47.315984Z",
     "start_time": "2025-03-09T04:49:30.598630Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"../data/training-dataset/pulls.jsonl\")\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1\"  # or another DeepSeek variant\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(data_record):\n",
    "    return tokenizer(data_record[\"title\"] + \" \" + data_record[\"body\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/deepseek\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,  # Enable mixed precision for faster training\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"] if \"test\" in tokenized_datasets else None,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for FP8 quantization.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     10\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer(data_record[\u001B[33m\"\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m\"\u001B[39m] + \u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m + data_record[\u001B[33m\"\u001B[39m\u001B[33mbody\u001B[39m\u001B[33m\"\u001B[39m], truncation=\u001B[38;5;28;01mTrue\u001B[39;00m, padding=\u001B[33m\"\u001B[39m\u001B[33mmax_length\u001B[39m\u001B[33m\"\u001B[39m, max_length=\u001B[32m512\u001B[39m)\n\u001B[32m     12\u001B[39m tokenized_datasets = dataset.map(preprocess_function)\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mauto\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m training_args = TrainingArguments(\n\u001B[32m     17\u001B[39m     output_dir=\u001B[33m\"\u001B[39m\u001B[33m../models/deepseek\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     18\u001B[39m     evaluation_strategy=\u001B[33m\"\u001B[39m\u001B[33mepoch\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     26\u001B[39m     push_to_hub=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m     27\u001B[39m )\n\u001B[32m     29\u001B[39m trainer = Trainer(\n\u001B[32m     30\u001B[39m     model=model,\n\u001B[32m     31\u001B[39m     args=training_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m     34\u001B[39m     tokenizer=tokenizer,\n\u001B[32m     35\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:559\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    557\u001B[39m     \u001B[38;5;28mcls\u001B[39m.register(config.\u001B[34m__class__\u001B[39m, model_class, exist_ok=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    558\u001B[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001B[32m--> \u001B[39m\u001B[32m559\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    560\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    561\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    562\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m._model_mapping.keys():\n\u001B[32m    563\u001B[39m     model_class = _get_model_class(config, \u001B[38;5;28mcls\u001B[39m._model_mapping)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:262\u001B[39m, in \u001B[36mrestore_default_torch_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    260\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    261\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m262\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    264\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3698\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   3695\u001B[39m     hf_quantizer = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   3697\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3698\u001B[39m     \u001B[43mhf_quantizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3699\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3700\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3701\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3702\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3703\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3704\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3705\u001B[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001B[32m   3706\u001B[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_finegrained_fp8.py:51\u001B[39m, in \u001B[36mFineGrainedFP8HfQuantizer.validate_environment\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     45\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m     46\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mConverting into FP8 weights from tf/flax weights is currently not supported, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     47\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mplease make sure the weights are in PyTorch format.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     48\u001B[39m     )\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch.cuda.is_available():\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mNo GPU found. A GPU is needed for FP8 quantization.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     53\u001B[39m compute_capability = torch.cuda.get_device_capability()\n\u001B[32m     54\u001B[39m major, minor = compute_capability\n",
      "\u001B[31mRuntimeError\u001B[39m: No GPU found. A GPU is needed for FP8 quantization."
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
