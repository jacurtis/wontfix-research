{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T04:40:30.888892Z",
     "start_time": "2025-03-09T04:40:29.777712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install transformers accelerate evaluate datasets peft -q\n",
    "!pip install ipywidgets sentencepiece protobuf"
   ],
   "id": "c4111c2da0de80c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (8.1.5)\r\n",
      "Requirement already satisfied: sentencepiece in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (0.2.0)\r\n",
      "Requirement already satisfied: protobuf in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (6.30.0)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipywidgets) (9.0.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\r\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\r\n",
      "Requirement already satisfied: decorator in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\r\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\r\n",
      "Requirement already satisfied: matplotlib-inline in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\r\n",
      "Requirement already satisfied: stack_data in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\r\n",
      "Requirement already satisfied: pure-eval in /Users/alexcurtis/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T04:41:03.984622Z",
     "start_time": "2025-03-09T04:40:30.893230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "# login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "# Load pre-trained LLaMA model\n",
    "print(\"Loading LLaMA model...\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "print(\"Loading LLaMA tokenizer...\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n"
   ],
   "id": "a96cc9d6572b1fd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "852ff8414e1446d49ef54dbe1bbe8eb9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA tokenizer...\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T04:41:04.525532Z",
     "start_time": "2025-03-09T04:41:04.127164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the training dataset\n",
    "def preprocess_function(data_record):\n",
    "    return tokenizer(data_record[\"title\"] + \" \" + data_record[\"body\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"../data/training-dataset/pulls.jsonl\")\n",
    "tokenized_datasets = dataset.map(preprocess_function)"
   ],
   "id": "4e99e74389d6b381",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c10f463db042df69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T04:42:18.925375Z",
     "start_time": "2025-03-09T04:41:04.535028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure LoRa\n",
    "config = LoraConfig(\n",
    "    r=8,  # Rank of the low-rank matrices\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Apply LoRa to specific layers\n",
    ")\n",
    "\n",
    "# Apply LoRa to the model\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# Set up training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/llama-2-7b\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, train_dataset=tokenized_datasets[\"train\"], args=training_args)\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ],
   "id": "47bfb1eac98d1643",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 20.26 GB, other allocations: 464.00 KB, max allowed: 20.40 GB). Tried to allocate 172.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# Set up training\u001B[39;00m\n\u001B[32m     13\u001B[39m training_args = TrainingArguments(\n\u001B[32m     14\u001B[39m     output_dir=\u001B[33m\"\u001B[39m\u001B[33m../models/llama-2-7b\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     15\u001B[39m     per_device_train_batch_size=\u001B[32m2\u001B[39m,\n\u001B[32m     16\u001B[39m     gradient_accumulation_steps=\u001B[32m2\u001B[39m,\n\u001B[32m     17\u001B[39m     num_train_epochs=\u001B[32m3\u001B[39m\n\u001B[32m     18\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m trainer = \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenized_datasets\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtraining_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# Fine-tune the model\u001B[39;00m\n\u001B[32m     23\u001B[39m trainer.train()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/transformers/trainer.py:612\u001B[39m, in \u001B[36mTrainer.__init__\u001B[39m\u001B[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001B[39m\n\u001B[32m    607\u001B[39m \u001B[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001B[39;00m\n\u001B[32m    608\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    609\u001B[39m     \u001B[38;5;28mself\u001B[39m.place_model_on_device\n\u001B[32m    610\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(model, \u001B[33m\"\u001B[39m\u001B[33mquantization_method\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) == QuantizationMethod.BITS_AND_BYTES\n\u001B[32m    611\u001B[39m ):\n\u001B[32m--> \u001B[39m\u001B[32m612\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_move_model_to_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    614\u001B[39m \u001B[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001B[39;00m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_model_parallel:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/transformers/trainer.py:899\u001B[39m, in \u001B[36mTrainer._move_model_to_device\u001B[39m\u001B[34m(self, model, device)\u001B[39m\n\u001B[32m    898\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_move_model_to_device\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, device):\n\u001B[32m--> \u001B[39m\u001B[32m899\u001B[39m     model = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    900\u001B[39m     \u001B[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001B[39;00m\n\u001B[32m    901\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.parallel_mode == ParallelMode.TPU \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[33m\"\u001B[39m\u001B[33mtie_weights\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001B[39m, in \u001B[36mModule.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1340\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1341\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1343\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    901\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    902\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m903\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    905\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    906\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    907\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    908\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    913\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    914\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    901\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    902\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m903\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    905\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    906\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    907\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    908\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    913\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    914\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[31m[... skipping similar frames: Module._apply at line 903 (4 times)]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    901\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    902\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m903\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    905\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    906\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    907\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    908\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    913\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    914\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    927\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    928\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    929\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    931\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    933\u001B[39m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Developer/WontfixAdmin/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001B[39m, in \u001B[36mModule.to.<locals>.convert\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1322\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t.dim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m):\n\u001B[32m   1323\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m t.to(\n\u001B[32m   1324\u001B[39m             device,\n\u001B[32m   1325\u001B[39m             dtype \u001B[38;5;28;01mif\u001B[39;00m t.is_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t.is_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1326\u001B[39m             non_blocking,\n\u001B[32m   1327\u001B[39m             memory_format=convert_to_format,\n\u001B[32m   1328\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1329\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1330\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1331\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1332\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1333\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1334\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1335\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) == \u001B[33m\"\u001B[39m\u001B[33mCannot copy out of meta tensor; no data!\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mRuntimeError\u001B[39m: MPS backend out of memory (MPS allocated: 20.26 GB, other allocations: 464.00 KB, max allowed: 20.40 GB). Tried to allocate 172.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
